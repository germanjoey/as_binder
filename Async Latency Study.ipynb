{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from pt.plotter import *\n",
    "from bqplot import LogScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "        var others0 = [0, 1, 2, 3];\n",
       "        var code_shown0 = true; \n",
       "        function code_toggle0 () {\n",
       "            var selector = \"div.input\";\n",
       "            var inputs = $(selector).toArray();\n",
       "            if (code_shown0) {\n",
       "                for (var i in others0) {\n",
       "                    var x = others0[i];\n",
       "                    $(inputs[x]).hide();\n",
       "                }\n",
       "            }\n",
       "            else {\n",
       "                for (var i in others0) {\n",
       "                    var x = others0[i];\n",
       "                    $(inputs[x]).show();\n",
       "                }\n",
       "            }\n",
       "\n",
       "            code_shown0 = !code_shown0;\n",
       "        } \n",
       "        $( document ).ready(code_toggle0);\n",
       "        </script>\n",
       "        <form action=\"javascript:code_toggle0()\">\n",
       "        <input type=\"submit\" value=\"Toggle on/off the display of the document setup code.\">\n",
       "        </form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML as DHTML\n",
    "\n",
    "# function to create those toggle buttons, adapted from something from stackoverflow\n",
    "def add_toggle_button(desc, *inputs):\n",
    "    x = ', '.join([str(x) for x in inputs])\n",
    "    n = str(inputs[0])\n",
    "    \n",
    "    code = '''\n",
    "        <script>\n",
    "        var others%n = [%x];\n",
    "        var code_shown%n = true; \n",
    "        function code_toggle%n () {\n",
    "            var selector = \"div.input\";\n",
    "            var inputs = $(selector).toArray();\n",
    "            if (code_shown%n) {\n",
    "                for (var i in others%n) {\n",
    "                    var x = others%n[i];\n",
    "                    $(inputs[x]).hide();\n",
    "                }\n",
    "            }\n",
    "            else {\n",
    "                for (var i in others%n) {\n",
    "                    var x = others%n[i];\n",
    "                    $(inputs[x]).show();\n",
    "                }\n",
    "            }\n",
    "\n",
    "            code_shown%n = !code_shown%n;\n",
    "        } \n",
    "        $( document ).ready(code_toggle%n);\n",
    "        </script>\n",
    "        <form action=\"javascript:code_toggle%n()\">\n",
    "        <input type=\"submit\" value=\"Toggle on/off the display of the %d code.\">\n",
    "        </form>'''\n",
    "    \n",
    "    return code.replace('%d', desc).replace('%n', str(n)).replace('%x', x)\n",
    "\n",
    "DHTML(add_toggle_button('document setup', 0, 1, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "/* prevent truncation of the slider labels */\n",
       ".widget-label, .widget-button, .jupyter-button, .widget-readout {\n",
       "    width: unset !important;\n",
       "    min-width: fit-content !important;\n",
       "}\n",
       ".widget-vbox > .widget-label {\n",
       "    font-weight: bolder;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "/* prevent truncation of the slider labels */\n",
    ".widget-label, .widget-button, .jupyter-button, .widget-readout {\n",
    "    width: unset !important;\n",
    "    min-width: fit-content !important;\n",
    "}\n",
    ".widget-vbox > .widget-label {\n",
    "    font-weight: bolder;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_options = {\n",
    "    'pre_resp_time': [0, 1e-3, 2e-3, 5e-3, 1e-2, 2e-2, 5e-2, 1e-1],\n",
    "    'pre_resp_txn_time': [0, 1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 1e-2, 2e-2, 5e-2],\n",
    "    'post_resp_time': [0, 1e-3, 2e-3, 5e-3, 1e-2, 2e-2, 5e-2, 1e-1],\n",
    "    'post_resp_txn_time': [0, 1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 1e-2, 2e-2, 5e-2],\n",
    "    'cpu_utilization_base': [60.0, 62.5, 65.0, 67.5, 70.0, 72.5, 75.0, 77.5, 80.0, 82.5, 85.0, 87.5, 90.0, 92.5, 95.0, 97.5],\n",
    "    'cpu_utilization_txn': [60.0, 62.5, 65.0, 67.5, 70.0, 72.5, 75.0, 77.5, 80.0, 82.5, 85.0, 87.5, 90.0, 92.5, 95.0, 97.5]\n",
    "}\n",
    "\n",
    "set_default_plotter_options(default_options)\n",
    "\n",
    "default_pre_resp_time = 5e-3\n",
    "default_pre_resp_txn_time = 1e-3\n",
    "default_post_resp_time = 5e-3\n",
    "default_post_resp_txn_time = 1e-3\n",
    "default_cpu_utilization_base = 75.0\n",
    "default_cpu_utilization_txn = 87.5\n",
    "\n",
    "minimum_rate = numpy.zeros(10000) + 1e-6\n",
    "maximum_rate = numpy.zeros(10000) + 10 ** 4\n",
    "xr = numpy.logspace(1, 4, 10000)\n",
    "\n",
    "min_cpu_utilization = default_options['cpu_utilization_base'][0]\n",
    "# min_cpu_utilization = 1 / (1 - default_cpu_utilization_base/100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Async Request/Response model\n",
    "\n",
    "\n",
    "```\n",
    "   users'             \n",
    "  responses               |----------|            |----------|            |----------|\n",
    "<------------------------ |response 1| <--------- |response 2| <--------- |response 3|\n",
    "                          |----------|            |----------|            |----------|\n",
    "\n",
    "                          ^                       ^                       ^\n",
    "                          |                       |                       |\n",
    "                          |                       |                       |\n",
    "                          |                       |                       |\n",
    "   users'\n",
    "  requests |----| |---------------| |----| |---------------| |----| |---------------|\n",
    "---------> |wait| |<- request 1 ->| |wait| |<- request 2 ->| |wait| |<- request 3 ->|\n",
    "           |----| |---------------| |----| |---------------| |----| |---------------|\n",
    "\n",
    "                  |               |\n",
    "                  |               |\n",
    "                 /                 \\\n",
    "                /                   \\\n",
    "               /                     \\\n",
    "              /                       \\\n",
    "             /                         \\\n",
    "            /                           \\\n",
    "           /                             \\\n",
    "          |                              |\n",
    "          |                              |\n",
    "          v                              v\n",
    "\n",
    "          |------------------------------|   pre-resp = \"execution time\" before response is emitted        \n",
    "          |              |               |   post-resp = remaining \"execution time\" after response is emitted\n",
    "          |<- pre-resp ->|<- post-resp ->|\n",
    "          |              |               |\n",
    "          |------------------------------|\n",
    "\n",
    "                         |\n",
    "                         |\n",
    "                         |\n",
    "                         v\n",
    "\n",
    "                     ----------\n",
    "                     |response|\n",
    "                     ----------\n",
    "\n",
    "\n",
    "\"execution time\" = customer logic processing time + new relic processing time + \"waiting time\"\n",
    "\"waiting time\" = total time spent idle waiting for execution\n",
    "                 NOTE: because of how the ioloop works (i.e. it interleaves), this will actually\n",
    "                 be \"distributed\" into the pre-resp period of the current request and post-resp\n",
    "                 period of the prior request(s). at lower input throughputs this \"waiting time\"\n",
    "                 will functionally disappear if the cpu utilization is low enough such that \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Waiting Time (Latency) for M/D/1 queue\n",
    "\n",
    "M/D/1 means \"markovian generator feeding into 1 deterministic consumer\"\n",
    "\n",
    "i.e. the M means that we're assuming that the time between requests is distributed according to a Poisson Distribution, and D means is that we're assuming the the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3556a9c33ac44b8fb3efe5b832b62ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade611731c64450e9dc12a759e89e8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Figure(axes=[Axis(label='Utilization Factor (arrival rate / service_rate)', scale=LinearScale(max=0.9899999999…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def waiting_time(x):\n",
    "    return 1 / (1 - x)\n",
    "\n",
    "x = numpy.linspace(0, 0.99, 100)\n",
    "fig = make_quick_plot(\n",
    "        x, waiting_time, \n",
    "        ax='Utilization Factor (arrival rate / service_rate)',\n",
    "        ay='Waiting Time Factor',\n",
    "        ysc_min=0,\n",
    "        ysc_max=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_latency(reqs_per_min, base_time, txn_time):\n",
    "    arrival_rate = reqs_per_min / 60.0\n",
    "    service_time = base_time + txn_time\n",
    "    service_rate = 1 / service_time\n",
    "    \n",
    "    # for M/M/1 Queue\n",
    "    # diff = numpy.maximum(minimum_rate, service_rate - arrival_rate)\n",
    "    # waiting_time = arrival_rate / (service_rate * diff)\n",
    "    \n",
    "    # for M/D/1 Queue\n",
    "    queue_utilization = numpy.minimum(1, arrival_rate / service_rate)\n",
    "    waiting_time = queue_utilization / (2 * service_rate * (1 - queue_utilization + 1e-6))\n",
    "    \n",
    "    waiting_time = numpy.minimum(maximum_rate, waiting_time)\n",
    "    return service_time + waiting_time\n",
    "\n",
    "def calc_cpu_overhead(total_time, cpu_utilization_factor):\n",
    "    cpu_overhead_percent = 1 / (1 - max(0, cpu_utilization_factor)/100.0)\n",
    "    return total_time * (cpu_overhead_percent ** 2)\n",
    "\n",
    "def calc_latency_with_overhead(reqs_per_min, base_time, txn_time, cpu_utilization_factor):\n",
    "    latency = calc_latency(reqs_per_min, base_time, txn_time)\n",
    "    return calc_cpu_overhead(latency, cpu_utilization_factor)\n",
    "\n",
    "def calc_throughput(reqs_per_min, pre_time, pre_txn_time, post_time, post_txn_time, cpu_utilization_factor):\n",
    "    latency = calc_latency(reqs_per_min, pre_time, pre_txn_time)\n",
    "    total_time = latency + post_time + post_txn_time\n",
    "    total_time_with_cpu_overhead = calc_cpu_overhead(total_time, cpu_utilization_factor)\n",
    "    return numpy.minimum(reqs_per_min, 60.0 / total_time_with_cpu_overhead)\n",
    "\n",
    "# to make percentile plots monotomically increasing despite req_rate > service_rate\n",
    "def set_percentile_ceiling(percentile, min_val, max_val):\n",
    "    found = False\n",
    "    for i in range(1, len(percentile)):\n",
    "        if percentile[i] > min_val:\n",
    "            found = True\n",
    "        if found and (percentile[i] < percentile[i-1]):\n",
    "            percentile[i] = max_val\n",
    "\n",
    "    return percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latency w/ Agent vs Latency w/o Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ee24d8eeb248f09e3598c3acefe408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(SelectionSlider(description='Pre_Resp_Time', index=3, options=(0, 0.001, 0.002, 0.005, 0.01, 0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f480670900764e5a88b8268663e831ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Figure(axes=[Axis(label='Requests Per Minute', scale=LogScale(max=10000.0, min=10.0)), Axis(label='Average Lat…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def no_agent_latency(x, pre_resp_time=1, pre_resp_txn_time=1, post_resp_time=1, post_resp_txn_time=1, cpu_utilization_base=1, cpu_utilization_txn=1):\n",
    "    return calc_latency_with_overhead(x, pre_resp_time, 0, 0)\n",
    "\n",
    "def with_agent_latency(x, pre_resp_time=1, pre_resp_txn_time=1, post_resp_time=1, post_resp_txn_time=1, cpu_utilization_base=1, cpu_utilization_txn=1):\n",
    "    cpu_utilization = cpu_utilization_txn - cpu_utilization_base\n",
    "    return calc_latency_with_overhead(x, pre_resp_time, pre_resp_txn_time, cpu_utilization)\n",
    "\n",
    "fig = make_quick_plot(\n",
    "        xr, with_agent_latency, no_agent_latency, \n",
    "        ax='Requests Per Minute',\n",
    "        ay=\"Average Latency\",\n",
    "        ysc_min=0.001,\n",
    "        ysc_max=1,\n",
    "        xscale=LogScale,\n",
    "        yscale=LogScale,\n",
    "        defaults={\n",
    "            'pre_resp_time': default_pre_resp_time,\n",
    "            'pre_resp_txn_time': default_pre_resp_txn_time,\n",
    "            'post_resp_time': default_post_resp_time,\n",
    "            'post_resp_txn_time': default_post_resp_txn_time,\n",
    "            'cpu_utilization_base': default_cpu_utilization_base,\n",
    "            'cpu_utilization_txn': default_cpu_utilization_txn,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percent Latency Increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1873e567a5c4498ab528a02a7e6360c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(SelectionSlider(description='Pre_Resp_Time', index=3, options=(0, 0.001, 0.002, 0.005, 0.01, 0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8191cec7bb4616a5a8d76d0dce2280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Figure(axes=[Axis(label='Requests Per Minute', scale=LogScale(max=10000.0, min=10.0)), Axis(label='Percent Lat…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lpo_ymax = 1000\n",
    "lpo_ymin = 2e-4\n",
    "\n",
    "def latency_percent_overhead(x, pre_resp_time=1, pre_resp_txn_time=1, post_resp_time=1, post_resp_txn_time=1, cpu_utilization_base=1, cpu_utilization_txn=1):\n",
    "    n = no_agent_latency(x, pre_resp_time, pre_resp_txn_time, post_resp_time, post_resp_txn_time, cpu_utilization_base, cpu_utilization_txn)\n",
    "    w = with_agent_latency(x, pre_resp_time, pre_resp_txn_time, post_resp_time, post_resp_txn_time, cpu_utilization_base, cpu_utilization_txn)\n",
    "    \n",
    "    overhead = 100 * numpy.abs((w - n) / n)\n",
    "    return set_percentile_ceiling(overhead, lpo_ymin, 2*lpo_ymax)\n",
    "   \n",
    "fig = make_quick_plot(\n",
    "        xr, latency_percent_overhead,\n",
    "        ax='Requests Per Minute',\n",
    "        ay=\"Percent Latency Increase\",\n",
    "        ysc_min=1,\n",
    "        xscale=LogScale,\n",
    "        yscale=LogScale,\n",
    "        ysc_max=lpo_ymax - 1,\n",
    "        defaults={\n",
    "            'pre_resp_time': default_pre_resp_time,\n",
    "            'pre_resp_txn_time': default_pre_resp_txn_time,\n",
    "            'post_resp_time': default_post_resp_time,\n",
    "            'post_resp_txn_time': default_post_resp_txn_time,\n",
    "            'cpu_utilization_base': default_cpu_utilization_base,\n",
    "            'cpu_utilization_txn': default_cpu_utilization_txn,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Throughput w/o Agent vs Throughput w/o Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8718aa1e4bb04042a86d9ca6529faf19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(SelectionSlider(description='Pre_Resp_Time', index=3, options=(0, 0.001, 0.002, 0.005, 0.01, 0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5dc6332f5a4ea4a14fd319de36a90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Figure(axes=[Axis(label='Requests Per Minute', scale=LogScale(max=10000.0, min=10.0)), Axis(label='Actual Thro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def no_agent_throughput(x, pre_resp_time=1, pre_resp_txn_time=1, post_resp_time=1, post_resp_txn_time=1, cpu_utilization_base=1, cpu_utilization_txn=1):\n",
    "    return calc_throughput(x, pre_resp_time, 0, post_resp_time, 0, 0)\n",
    "    \n",
    "def with_agent_throughput(x, pre_resp_time=1, pre_resp_txn_time=1, post_resp_time=1, post_resp_txn_time=1, cpu_utilization_base=1, cpu_utilization_txn=1):\n",
    "    cpu_utilization = cpu_utilization_txn - cpu_utilization_base\n",
    "    return calc_throughput(x, pre_resp_time, pre_resp_txn_time, post_resp_time, post_resp_txn_time, cpu_utilization)\n",
    "\n",
    "fig = make_quick_plot(\n",
    "        xr, with_agent_throughput, no_agent_throughput, \n",
    "        ax='Requests Per Minute',\n",
    "        ay='Actual Throughput',\n",
    "        xscale=LogScale,\n",
    "        yscale=LogScale,\n",
    "        ysc_min=10,\n",
    "        ysc_max=10000,\n",
    "        defaults={\n",
    "            'pre_resp_time': default_pre_resp_time,\n",
    "            'pre_resp_txn_time': default_pre_resp_txn_time,\n",
    "            'post_resp_time': default_post_resp_time,\n",
    "            'post_resp_txn_time': default_post_resp_txn_time,\n",
    "            'cpu_utilization_base': default_cpu_utilization_base,\n",
    "            'cpu_utilization_txn': default_cpu_utilization_txn,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percent Throughput Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef65c877ef984bb1866d782dd2e7e847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(SelectionSlider(description='Pre_Resp_Time', index=3, options=(0, 0.001, 0.002, 0.005, 0.01, 0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b40a48dd1564960b8c9167bf3b05909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Figure(axes=[Axis(label='Requests Per Minute', scale=LogScale(max=10000.0, min=10.0)), Axis(label='Percent Thr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tpr_ymax = 100\n",
    "tpr_ymin = 2e-4\n",
    "\n",
    "def throughput_percent_degradation(x, pre_resp_time=1, pre_resp_txn_time=1, post_resp_time=1, post_resp_txn_time=1, cpu_utilization_base=1, cpu_utilization_txn=1):\n",
    "    n = no_agent_throughput(x, pre_resp_time, pre_resp_txn_time, post_resp_time, post_resp_txn_time, cpu_utilization_base, cpu_utilization_txn)\n",
    "    w = with_agent_throughput(x, pre_resp_time, pre_resp_txn_time, post_resp_time, post_resp_txn_time, cpu_utilization_base, cpu_utilization_txn)\n",
    "    \n",
    "    percent_degradation = 100 * numpy.abs((w - n) / (w+n))\n",
    "    return set_percentile_ceiling(percent_degradation, tpr_ymin, 2*tpr_ymax)\n",
    "    \n",
    "fig = make_quick_plot(\n",
    "        xr, throughput_percent_degradation,\n",
    "        ax='Requests Per Minute',\n",
    "        ay=\"Percent Throughput Degradation\",\n",
    "        ysc_min=0,\n",
    "        ysc_max=tpr_ymax - 1,\n",
    "        xscale=LogScale,\n",
    "        defaults={\n",
    "            'pre_resp_time': default_pre_resp_time,\n",
    "            'pre_resp_txn_time': default_pre_resp_txn_time,\n",
    "            'post_resp_time': default_post_resp_time,\n",
    "            'post_resp_txn_time': default_post_resp_txn_time,\n",
    "            'cpu_utilization_base': default_cpu_utilization_base,\n",
    "            'cpu_utilization_txn': default_cpu_utilization_txn,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
